{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omEQHVdOS61G"
   },
   "source": [
    "# Coursework: Machine Translation Evaluation\n",
    "\n",
    "This notebook presents the different embeddings and regression techniques described in the report. Note word2vec embeddings are not presented as the results were so bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZaEAx4M49Nj"
   },
   "source": [
    "\n",
    "## Machine Translation Evaluation\n",
    "\n",
    "\n",
    "The code provided is split into the follwing sections:\n",
    "\n",
    "> Importing data and setting up GPU\n",
    "\n",
    "> Computing sentence embeddings using the 'FLAIR' library: pre-processing the data and getting the training and validation sets\n",
    "\n",
    "> Computing embeddings using the pre-trained BERT: pre-processing the data and getting the training and validation sets\n",
    "\n",
    "> Pytorch Neural Network Regressor \n",
    "\n",
    "> Other Regressors that we experimented with\n",
    "\n",
    "> Writing results into a .txt file\n",
    "\n",
    "##### Tech\n",
    "\n",
    "Our code uses the follwing libraries:\n",
    "\n",
    "* IO module - provided Python interfaces for stream handling\n",
    "* Torch - a library for creating neural networks and data handling\n",
    "* Flair - a library for computing sentence embeddings \n",
    "* Scipy - a stats based library for data evaluation\n",
    "* nltk - a library specialising in natural language processing \n",
    "* spacy - a library for data handling \n",
    "* transformers - a library such that we can import BERT modules\n",
    "* SKLearn - a library to train regressors and neural networks \n",
    "* Keras - a library specilalising in building neural networks \n",
    "\n",
    "##### Installation\n",
    "\n",
    "All of the above libraries need to be installed in order to run our .ipynb file. We have included all of the install commands in the file, therefore if you run each cell one after another each library will be installed. \n",
    "\n",
    "##### Instructions for code to be run \n",
    "\n",
    "The code is in a .ipynb file, which is in jupyter-notebook form. The code is split into sections that are meant to be run in chronological order. If one does not run the code in the correct order then necessary packages/libraries will not be installed/imported and the code will not run. I have split how to run the code into the follwing steps:\n",
    "\n",
    "* The first step is to download the required data that will be using, which inclued the English German sentences, as well as the associated translation scores. \n",
    "* Next there is the data manipulation using NLP techniques and the BERT and FLAIR libraries. This section uses techniques to make the data easier to manipulate with our regressors, which will help us to evaluate the translations. \n",
    "* To use FLAIR embeddings run all of the cells under the 'Computing Sentence Embeddings - FLAIR library'. FLAIR embeddings only return vectors.\n",
    "* To get BERT embeddings run all cells under 'Computing Embeddings - pretrained BERT'\n",
    "* To run PyTorch NN regressor run all cells under 'Pytorch NN Regressor'\n",
    "* The PyTorch NN regressor requires vector inputs. To return vector from the BERT embedding set pooling_fn=None\n",
    "> de_train_mt = get_bert_embeddings(\"./train.ende.mt\",'de', pooling_fcn=None)\n",
    "* To run other regression methods run cells under 'Other regressors'. \n",
    "* Regressors in 'Other regressors' all take scalar inputs. To return scalar inputs from BERT embeddings set pooling_fn=torch.mean\n",
    "> de_train_mt = get_bert_embeddings(\"./train.ende.mt\",'de', pooling_fcn=None)\n",
    "* Finally, we write our results into a .txt file which can be uploaded to the competition website Codalab. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lweXud1Wpemd"
   },
   "source": [
    "## A. English-German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yu6s3YOf_C93"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scs7ICZrPFcs"
   },
   "outputs": [],
   "source": [
    "# Download and unzip the data\n",
    "from os.path import exists\n",
    "if not exists('ende_data.zip'):\n",
    "    !wget - O ende_data.zip https: // competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
    "    !unzip ende_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "jPy_iwHnOSAZ",
    "outputId": "9b29c373-964d-4c53-fb6b-48941fadcd8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-DE---\n",
      "\n",
      "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
      "\n",
      "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
      "\n",
      "Score:  1.1016968715664406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the files\n",
    "import io\n",
    "\n",
    "# English-German\n",
    "print(\"---EN-DE---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.ende.src\", \"r\") as ende_src:\n",
    "    print(\"Source: \", ende_src.readline())\n",
    "with open(\"./train.ende.mt\", \"r\") as ende_mt:\n",
    "    print(\"Translation: \", ende_mt.readline())\n",
    "with open(\"./train.ende.scores\", \"r\") as ende_scores:\n",
    "    print(\"Score: \", ende_scores.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDm3eqQwFahr"
   },
   "source": [
    "### Setting up GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B2MuiNrrFZQf",
    "outputId": "0e100650-1ac4-4b96-c475-1fc7ae62f888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Fix GPU seeds\n",
    "SEED = 9320\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print('Device is', device)\n",
    "\n",
    "\n",
    "# we fix the seeds to get consistent results before every training\n",
    "# loop in what follows\n",
    "def fix_seed(seed=234):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "fix_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiFHVnfH_Jpv"
   },
   "source": [
    "### Computing Sentence Embeddings - FLAIR library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g05fv5GiSyQ4"
   },
   "source": [
    "Here we use the [FLAIR](https://github.com/flairNLP/flair) library, to compute word embeddings in both German and English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "96bRtBbuZLJe",
    "outputId": "9ab6e5f2-ceb2-4218-e99d-8b52bc3178af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.7)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.4.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.6.1)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.3)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.7)\n",
      "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
      "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (5.3.5)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.1)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Requirement already satisfied: transformers>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from flair) (2.5.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.7)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->flair) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (1.17.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (0.1.85)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.1.8)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.8.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.5.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.13.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (0.0.38)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (1.11.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair) (45.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.0.0)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.3.0->flair) (7.0)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers>=2.3.0->flair) (0.15.2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "import flair\n",
    "import torch\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4xFHULbu-cB-",
    "outputId": "6c010597-487a-4b4c-b04a-e290b2f63995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
      "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
      "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.3.5)\n",
      "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.8)\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.11.15)\n",
      "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
      "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
      "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.3)\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.9.2)\n",
      "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
      "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.1)\n",
      "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.15.0)\n",
      "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.11)\n",
      "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
      "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
      "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
      "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
      "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.13.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (20.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.1.8)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.11.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
      "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.85)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX>=1.2->allennlp) (45.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->allennlp) (3.0.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->allennlp) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.embeddings import CharacterEmbeddings\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.embeddings import BertEmbeddings\n",
    "from flair.embeddings import ELMoEmbeddings\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "!pip install allennlp\n",
    "\n",
    "###########English Embeddings##########\n",
    "# glove_embedding = WordEmbeddings('glove')\n",
    "# character_embeddings = CharacterEmbeddings()\n",
    "# bert_embedding = BertEmbedding()\n",
    "elmo_embedding = ELMoEmbeddings()\n",
    "\n",
    "flair_forward_en = FlairEmbeddings('news-forward-fast')\n",
    "flair_backward_en = FlairEmbeddings('news-backward-fast')\n",
    "\n",
    "###########German Embeddings###############\n",
    "\n",
    "distillBERT_de = BertEmbeddings(\n",
    "    bert_model_or_path=\"distilbert-base-german-cased\")\n",
    "BERT_de = BertEmbeddings(bert_model_or_path=\"bert-base-german-cased\")\n",
    "\n",
    "\n",
    "#################MultiLingual Embeddings##########\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
    "bert_embedding2 = BertEmbeddings(bert_model_or_path=\"albert-base-v2\")\n",
    "\n",
    "# The code below experimented with stacking embeddings\n",
    "\n",
    "# #Stack some embeddings:\n",
    "# stacked_embeddings = StackedEmbeddings(\n",
    "#     embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])\n",
    "\n",
    "\n",
    "# document_embeddings = DocumentPoolEmbeddings(\n",
    "#     embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])\n",
    "\n",
    "# #sentence = Sentence('The grass is green .')\n",
    "\n",
    "# # just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "# stacked_embeddings.embed(sentence)\n",
    "\n",
    "# # now check out the embedded tokens.\n",
    "# for token in sentence:\n",
    "#     print(token)\n",
    "#     print(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnfq7691c9AN"
   },
   "outputs": [],
   "source": [
    "document_embeddings_de = DocumentPoolEmbeddings(\n",
    "    embeddings=[BERT_de])\n",
    "\n",
    "\n",
    "document_embeddings_en = DocumentPoolEmbeddings(\n",
    "    embeddings=[elmo_embedding])\n",
    "\n",
    "\n",
    "# document_embeddings_de = DocumentPoolEmbeddings(\n",
    "#     embeddings=[bert_embedding])\n",
    "\n",
    "# document_embeddings_en  = DocumentPoolEmbeddings(\n",
    "#     embeddings=[bert_embedding ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zby8TQhgk8_q"
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import dct\n",
    "\n",
    "# DCT Pooling\n",
    "\n",
    "\n",
    "def DCT_pooling(sent, k=2):\n",
    "    '''\n",
    "    Calculates sentence embedding by saving first k coefficients of DCT transform\n",
    "    input: sent -> np array [B,N,D] B= Batch size N = number words, D = embedding dim\n",
    "           k - how many coefficients to keep\n",
    "    output: Sentence embedding\n",
    "    '''\n",
    "\n",
    "    num_words = sent.shape[0]\n",
    "    embedding_dim = sent.shape[1]\n",
    "    # DCT\n",
    "    out = dct(sent, type=3, n=k, axis=1)\n",
    "    # reshape into row vector\n",
    "    return out.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om6kQX5bX2mB"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhT2I6WYavY4"
   },
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PGRSOAdkAiGy",
    "outputId": "d2beac03-b315-4aee-924c-20b279e6bb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "9IesVgRhBOR6",
    "outputId": "bf58e851-a49c-4a3a-ac1e-09640190bbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# from nltk.stem.cistem import Cistem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading stopwords from the nltk package\n",
    "nltk.download('stopwords')  # stopwords dictionary, run once\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence_en(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if word not in stop_words_en:\n",
    "            if tag is None:\n",
    "                res_words.append(word)\n",
    "            else:\n",
    "                res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)\n",
    "\n",
    "\n",
    "def lemmatize_sentence_de(sentence):\n",
    "    '''\n",
    "    input: sentence (list(tokens))\n",
    "    return: lemmatized sentence list(tokens)\n",
    "    '''\n",
    "    stemmer = nltk.stem.cistem.Cistem()\n",
    "    # Assumes tokenizing first\n",
    "\n",
    "    return [stemmer.segment(token)[0] for token in sentence if token not in stop_words_de]\n",
    "\n",
    "\n",
    "def tokenize_sentences(corpus):\n",
    "\n",
    "    return [tokenizer.tokenize(s) for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19gsNCgnW8ZT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading stopwords from the nltk package\n",
    "# download('stopwords') #stopwords dictionary, run once\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "\n",
    "def get_sentence_emb(line, nlp, lang):\n",
    "    if lang == 'en':\n",
    "        # text = line.lower()\n",
    "        text = line\n",
    "        l = lemmatize_sentence_en(text)\n",
    "    elif lang == 'de':\n",
    "        # text = line.lower()\n",
    "        text = line\n",
    "        # l = lemmatize_sentence_de(text)\n",
    "        l = text\n",
    "        l = ' '.join([word for word in l if word not in stop_words_de])\n",
    "\n",
    "    sentence = Sentence(l)\n",
    "    nlp.embed(sentence)\n",
    "    return sentence.get_embedding()\n",
    "\n",
    "\n",
    "def get_embeddings(f, nlp, lang):\n",
    "    file = open(f)\n",
    "    lines = file.readlines()\n",
    "    sentences_vectors = []\n",
    "    count = 0\n",
    "    for l in lines:\n",
    "        vec = get_sentence_emb(l, nlp, lang)\n",
    "        if vec is not None:\n",
    "            # vec = np.mean(vec.cpu().detach().numpy())\n",
    "            sentences_vectors.append(vec.cpu().detach().numpy())\n",
    "        else:\n",
    "            print(\"didn't work :\", l)\n",
    "            sentences_vectors.append(0)\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUKMgbo2sreI"
   },
   "source": [
    "#### Getting Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXqZamIKs30T"
   },
   "source": [
    "We will now run the code fo the English-German translations and getting our training and validation sets ready for the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyJr7cIkQ3E8"
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp_de =spacy.load('de300')\n",
    "# nlp_en =spacy.load('en300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LwoUIDj0otbf",
    "outputId": "c403e81d-93db-4809-cdce-2b89dc290043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "English dev Done\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "German Done\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# EN-DE files\n",
    "de_train_src = get_embeddings(\"./train.ende.src\", document_embeddings_en, 'en')\n",
    "print(\"English dev Done\")\n",
    "de_train_mt = get_embeddings(\"./train.ende.mt\", document_embeddings_de, 'de')\n",
    "print('German Done')\n",
    "f_train_scores = open(\"./train.ende.scores\", 'r')\n",
    "de_train_scores = f_train_scores.readlines()\n",
    "\n",
    "de_val_src = get_embeddings(\"./dev.ende.src\", document_embeddings_en, 'en')\n",
    "de_val_mt = get_embeddings(\"./dev.ende.mt\", document_embeddings_de, 'de')\n",
    "f_val_scores = open(\"./dev.ende.scores\", 'r')\n",
    "de_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "U_K1CHl5VxiE",
    "outputId": "c3b5032b-245f-4beb-a530-a893f3274bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mt: 7000 Training src: 7000\n",
      "\n",
      "Validation mt: 1000 Validation src: 1000\n"
     ]
    }
   ],
   "source": [
    "# EN-DE\n",
    "print(f\"Training mt: {len(de_train_mt)} Training src: {len(de_train_src)}\")\n",
    "print()\n",
    "print(f\"Validation mt: {len(de_val_mt)} Validation src: {len(de_val_src)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Px7ikaGoy9r0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUPrCwMxFvpj"
   },
   "source": [
    "### Computing embeddings - pre-trained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3li9kN2q7Z5"
   },
   "source": [
    "Here, we implement a class that should serve as our BERT-preprocessing; it will provide the basis for a function that takes in a file name, a language, and optinonally either a pooling function or \"None\" as a pooling function (in which case it should return the un-pooled original vector embeddings) and return the BERT-embedding.\n",
    "\n",
    "\n",
    "To manage these tasks, it implements methods to prepare the data for BERT (tokenizing, adding special tokens, padding, generating attention masks) and a function to run the prepared data through the appropriate BERT model, plus applying the slected pooling method if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "eHbroJV7FKEH",
    "outputId": "34edcb4a-b6dc-422b-9327-81a5b257e581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
      "Device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertModel, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, sampler\n",
    "from torchtext import data, datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "!pip install transformers\n",
    "\n",
    "\n",
    "class BERTembedd(nn.Module):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super().__init__()\n",
    "        # Download the pre-trained BERT models and tokenizers for both English and German.\n",
    "        self.english_BERT = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.german_BERT = BertModel.from_pretrained('bert-base-german-cased')\n",
    "        self.english_tokenizer = BertTokenizer.from_pretrained(\n",
    "            'bert-base-uncased')\n",
    "        self.german_tokenizer = BertTokenizer.from_pretrained(\n",
    "            'bert-base-german-cased')\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Check whether GPU can be used.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            self.device = 'cuda:0'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        print('Device is', self.device)\n",
    "\n",
    "    def prepare_data(self, f, lang, max_len=30):\n",
    "        \"\"\"\n",
    "        This method prepares the data found in the file at location f and in language lang (either \"de\" or \"en\").\n",
    "        max_len can be specified; all tokenized sentences will be padded to this length.\n",
    "        For the provided dataset, 30 makes sense since no sentence in the dataset is longer, but some come close.\n",
    "\n",
    "        The method returns a dataloader with the sentences (tupled together with their respective masks)\n",
    "        prepared in a way so as to be ready to be fed into a BERT model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Read the data and set the tokenizer to the correct language\n",
    "        with open(f) as file:\n",
    "            lines = file.readlines()\n",
    "            if lang == \"en\":\n",
    "                tokenizer = self.english_tokenizer\n",
    "            elif lang == \"de\":\n",
    "                tokenizer = self.german_tokenizer\n",
    "            else:\n",
    "                raise ValueError(\"lang must be either en or de\")\n",
    "\n",
    "        # Tokenize the sentences in the dataset\n",
    "        input_ids = torch.LongTensor(\n",
    "            [tokenizer.encode(text, max_length=max_len, add_special_tokens=True, pad_to_max_length=True) for text in\n",
    "             lines])\n",
    "        # The tokenizer is now not needed any longer\n",
    "        tokenizer = None\n",
    "\n",
    "        # Create attention masks. They should record which tokens are \"genuine\" and which are padding.\n",
    "        attention_masks = []\n",
    "        attention_masks = torch.zeros(input_ids.shape).long()\n",
    "\n",
    "        # Mask of the token is 0 if token_id is 0 (padding). mask is 1 otherwise.\n",
    "        attention_masks[attention_masks != input_ids] = 1\n",
    "\n",
    "        # Create dataset and dataloader\n",
    "        dataset = list(zip(input_ids, attention_masks))\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def get_embeddings(self, f, lang, pooling_fcn=torch.mean):\n",
    "        \"\"\"\n",
    "        This method takes in a file path f, and a language lang (either \"de\" or \"en\") and returns\n",
    "        a list containing the embeddings for all the sentences.\n",
    "        An optional input is a pooling function: If this is set to None, the sentence embeddings \n",
    "        will be vectors of the dimension max_len (default: 30); if a pooling function is specified,\n",
    "        it will be used to reduce the embeddings to scalars.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the data for BERT\n",
    "        d = self.prepare_data(f, lang)\n",
    "\n",
    "        # Select the BERT model for the correct language\n",
    "        if lang == \"en\":\n",
    "            bert = self.english_BERT\n",
    "        elif lang == \"de\":\n",
    "            bert = self.german_BERT\n",
    "        else:\n",
    "            raise ValueError(\"lang must be de or en.\")\n",
    "\n",
    "        # Send the model to gpu if available.\n",
    "        bert = bert.to(self.device)\n",
    "\n",
    "        # Since we are not trainig our BERT model, we save RAM by excluding gradients.\n",
    "        bert.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # This list will be used to record the sentence embeddings\n",
    "            results = []\n",
    "\n",
    "            # Go through the batches of sentences with their masks provided by the dataloader.\n",
    "            for x in d:\n",
    "\n",
    "                # Extract the sentences and corresponding masks, and send them to the gpu if available\n",
    "                sentences = x[0].to(self.device)\n",
    "                masks = x[1].to(self.device)\n",
    "\n",
    "                # Run them through the model\n",
    "                result = bert(input_ids=sentences, attention_mask=masks)[\n",
    "                    0]  # -> (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "                # BERT outputs a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "                # In order to get a tensor of size (batch_size, sequence_length), we have to pool along the hidden_size\n",
    "\n",
    "                # pooled = F.max_pool1d(result, result.shape[2]).squeeze() # -> (batch_size, sequence_length, 1)\n",
    "                # -> (batch_size, sequence_length, 1)\n",
    "                pooled = F.avg_pool1d(result, result.shape[2]).squeeze()\n",
    "\n",
    "                # At this point, the shape of the pooled tensor is (batch_size, sequence_length).\n",
    "\n",
    "                # If futher pooling was desired (i.e. pooling_fcn was not set to None), we do it here and\n",
    "                # reduce the tensor to the shape (batch_size, 1)\n",
    "                if pooling_fcn is not None:\n",
    "                    sentence_vectors = pooling_fcn(pooled, 1)\n",
    "                else:\n",
    "                    sentence_vectors = pooled\n",
    "                # Add the batch to the list\n",
    "                results += list(sentence_vectors.cpu().numpy())\n",
    "\n",
    "        return results\n",
    "\n",
    "    def forward(self, f, lang, pooling_fcn=torch.mean):\n",
    "        return self.get_embeddings(f, lang, pooling_fcn)\n",
    "\n",
    "\n",
    "# Create a bert embedder for later use\n",
    "bertembedder = BERTembedd()\n",
    "\n",
    "# Define a simple-to use bert embedding function that you just need to plug\n",
    "# file name and languag in (and optinally polling function)\n",
    "# to directly get the embedding of the sentences in the file.\n",
    "\n",
    "\n",
    "def get_bert_embeddings(f, lang, pooling_fcn=torch.mean):\n",
    "    return bertembedder.forward(f, lang, pooling_fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rb4YrhbcFvOu"
   },
   "source": [
    "###### Bert embedding, getting training and validation with BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K9iPfjYFh-n"
   },
   "outputs": [],
   "source": [
    "# EN-DE files\n",
    "de_train_src = get_bert_embeddings(\"./train.ende.src\", 'en', pooling_fcn=None)\n",
    "\n",
    "de_train_mt = get_bert_embeddings(\"./train.ende.mt\", 'de', pooling_fcn=None)\n",
    "\n",
    "f_train_scores = open(\"./train.ende.scores\", 'r')\n",
    "de_train_scores = f_train_scores.readlines()\n",
    "\n",
    "de_val_src = get_bert_embeddings(\"./dev.ende.src\", 'en', pooling_fcn=None)\n",
    "de_val_mt = get_bert_embeddings(\"./dev.ende.mt\", 'de', pooling_fcn=None)\n",
    "f_val_scores = open(\"./dev.ende.scores\", 'r')\n",
    "de_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSIE7d8HCTpi"
   },
   "source": [
    "### Pytorch NN Regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-1j8PsLN6bY"
   },
   "source": [
    "##### Prepare test and validation set vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TiWuTeqoHemB",
    "outputId": "0c5adfbf-9e9e-4046-ddfb-9e3850e0a366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1016968715664406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put the features into a tesnor [B,D] - number sentences, embedding dim\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_samples = len(de_train_src)\n",
    "num_dims = len(de_train_src[0]) * 2\n",
    "X_train = torch.zeros((num_samples, num_dims), dtype=torch.float)\n",
    "for i in range(len(de_train_src)):\n",
    "    en_vec = de_train_src[i]\n",
    "    de_vec = de_train_mt[i]\n",
    "    vec = np.concatenate((en_vec, de_vec))\n",
    "    vec = torch.tensor(vec, dtype=torch.float).squeeze()\n",
    "\n",
    "    X_train[i, :] = vec\n",
    "\n",
    "X_train_de = X_train\n",
    "\n",
    "\n",
    "num_samples = len(de_val_src)\n",
    "num_dims = len(de_val_src[0]) * 2\n",
    "\n",
    "X_val = torch.zeros((num_samples, num_dims), dtype=torch.float)\n",
    "for i in range(len(de_val_src)):\n",
    "    en_vec = de_val_src[i]\n",
    "    de_vec = de_val_mt[i]\n",
    "    vec = np.concatenate((en_vec, de_vec))\n",
    "    vec = torch.tensor(vec, dtype=torch.float).squeeze()\n",
    "    X_val[i, :] = vec\n",
    "\n",
    "X_val_de = X_val\n",
    "\n",
    "\n",
    "# Scores\n",
    "print(de_train_scores[0])\n",
    "de_train_scores = np.array(de_train_scores, dtype=np.float)\n",
    "train_scores = torch.tensor(de_train_scores).type(torch.float)\n",
    "y_train_de = train_scores\n",
    "\n",
    "val_scores = np.array(de_val_scores, dtype=np.float)\n",
    "val_scores = torch.tensor(val_scores, dtype=torch.float)\n",
    "y_val_de = val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eoY14lNCTe3"
   },
   "source": [
    "Pytorch FCL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SvzW81Ezgak1",
    "outputId": "c4e6c1d6-94de-44bd-b9af-b0d6d3a5f904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "# setting a random seed to ensure that your results are reproducible.\n",
    "torch.manual_seed(0)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "print(\"Using GPU: {}\".format(use_cuda))\n",
    "\n",
    "\n",
    "class OneHiddenLayerRegressor(nn.Module):\n",
    "    # Define entities containing model weights in the constructor.\n",
    "    def __init__(self, n_hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(\n",
    "            in_features=num_dims, out_features=n_hidden, bias=True\n",
    "        )\n",
    "        self.linear2 = nn.Linear(\n",
    "            in_features=n_hidden, out_features=200, bias=True\n",
    "        )\n",
    "\n",
    "        self.linear3 = nn.Linear(\n",
    "            in_features=200, out_features=100, bias=True\n",
    "        )\n",
    "\n",
    "        self.linear4 = nn.Linear(\n",
    "            in_features=100, out_features=1, bias=True\n",
    "        )\n",
    "\n",
    "    # implements a `forward` method to define the\n",
    "    # computation that takes place on the forward pass. A corresponding\n",
    "    # `backward` method, which computes gradients, is automatically defined\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.linear1(inputs)\n",
    "        h = F.tanh(h)\n",
    "        h = self.linear2(h)\n",
    "        h = F.tanh(h)\n",
    "        h = self.linear3(h)\n",
    "        h = F.tanh(h)\n",
    "        h = self.linear4(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, log_interval=100):\n",
    "    \"\"\"\n",
    "    A utility function that performs a basic training loop.\n",
    "\n",
    "    For each batch in the training set, fetched using `train_loader`:\n",
    "        - Zeroes the gradient used by `optimizer`\n",
    "        - Performs forward pass through `model` on the given batch\n",
    "        - Computes loss on batch\n",
    "        - Performs backward pass\n",
    "        - `optimizer` updates model parameters using computed gradient\n",
    "\n",
    "    Prints the training loss on the current batch every `log_interval` batches.\n",
    "    \"\"\"\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # send our batch to the device we are using.\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zeroes the gradient used by `optimizer`; NOTE: if this is not done,\n",
    "        # then gradients will be accumulated across batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Performs forward pass through `model` on the given batch; equivalent\n",
    "        # to `model.forward(inputs)`.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Computes loss on batch; `F.mse_loss` computes the mean squared error\n",
    "        # loss on batch.\n",
    "\n",
    "        loss = F.mse_loss(outputs.squeeze(), targets)\n",
    "\n",
    "        # Performs backward pass; steps backward through the computation graph,\n",
    "        # computing the gradient of the loss wrt model parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # `optimizer` updates model parameters using computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Prints the training loss on the current batch every `log_interval`\n",
    "        # batches.\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {:02d} -- Batch: {:03d} -- Loss: {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx,\n",
    "\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def val(model, test_loader):\n",
    "    \"\"\"\n",
    "    A utility function to compute the loss and accuracy on a test set by\n",
    "    iterating through the test set using the provided `test_loader` and\n",
    "    accumulating the loss and accuracy on each batch.\n",
    "    \"\"\"\n",
    "    test_loss = 0.0\n",
    "    count = 0\n",
    "    test_pearson = 0\n",
    "    # Uses `torch.no_grad()` when to perform a forward pass\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # We use `reduction=\"sum\"` to aggregate losses across batches using\n",
    "            # summation instead of taking the mean - we will take the mean at\n",
    "            # the end once we have accumulated all the losses.\n",
    "            outputs = model(inputs)\n",
    "            test_loss += F.mse_loss(outputs.squeeze(),\n",
    "                                    targets, reduction=\"sum\").item()\n",
    "\n",
    "            pred = outputs\n",
    "\n",
    "            test_pearson = pearsonr(targets.cpu(), outputs.squeeze().cpu())\n",
    "            print(test_pearson)\n",
    "            count += 1\n",
    "\n",
    "    pearson_score = test_pearson  # return pearson score\n",
    "    print(f'Pearson score: {pearson_score[0]}')\n",
    "\n",
    "\n",
    "# Test function, which just does a forward pass through network\n",
    "def test(model, input):\n",
    "    with torch.no_grad():\n",
    "        return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaQblN_NgvYZ"
   },
   "source": [
    "Create data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_fuPsYAguWt"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders - batching input and test\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(\n",
    "    X_train_de, y_train_de), batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(\n",
    "    X_val_de, y_val_de), batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k59OhSXIhCAb"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "wayI9jpdhBLx",
    "outputId": "716ed66d-ec4d-47f8-aaa0-0276635335af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 00 -- Batch: 000 -- Loss: 0.4586\n",
      "Train Epoch: 01 -- Batch: 000 -- Loss: 0.5612\n",
      "Train Epoch: 02 -- Batch: 000 -- Loss: 0.8768\n",
      "Train Epoch: 03 -- Batch: 000 -- Loss: 0.6544\n",
      "Train Epoch: 04 -- Batch: 000 -- Loss: 0.6293\n",
      "Train Epoch: 05 -- Batch: 000 -- Loss: 0.3785\n",
      "Train Epoch: 06 -- Batch: 000 -- Loss: 0.4061\n",
      "Train Epoch: 07 -- Batch: 000 -- Loss: 0.4840\n",
      "Train Epoch: 08 -- Batch: 000 -- Loss: 0.4908\n",
      "Train Epoch: 09 -- Batch: 000 -- Loss: 0.5139\n",
      "Train Epoch: 10 -- Batch: 000 -- Loss: 0.6650\n",
      "Train Epoch: 11 -- Batch: 000 -- Loss: 1.1157\n",
      "Train Epoch: 12 -- Batch: 000 -- Loss: 1.2115\n",
      "Train Epoch: 13 -- Batch: 000 -- Loss: 0.5158\n",
      "Train Epoch: 14 -- Batch: 000 -- Loss: 0.5611\n",
      "Train Epoch: 15 -- Batch: 000 -- Loss: 0.6399\n",
      "Train Epoch: 16 -- Batch: 000 -- Loss: 1.1439\n",
      "Train Epoch: 17 -- Batch: 000 -- Loss: 0.5830\n",
      "Train Epoch: 18 -- Batch: 000 -- Loss: 0.7984\n",
      "Train Epoch: 19 -- Batch: 000 -- Loss: 0.4746\n",
      "(0.05836386431872912, 0.06505327125065499)\n",
      "Pearson score: 0.05836386431872912\n"
     ]
    }
   ],
   "source": [
    "model = OneHiddenLayerRegressor(n_hidden=100).to(device)\n",
    "\n",
    "# Create instance of optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train-test loop\n",
    "for epoch in range(20):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "# Return validation score\n",
    "val(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlF9jpCDguuV"
   },
   "source": [
    "### Other regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBgh9xS0PTzD"
   },
   "source": [
    "Putting data in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyfL6g5WPW34"
   },
   "outputs": [],
   "source": [
    "# Put the features into a list\n",
    "import numpy as np\n",
    "\n",
    "X_train = [np.array(de_train_src), np.array(de_train_mt)]\n",
    "X_train_de = np.array(X_train).transpose()\n",
    "\n",
    "X_val = [np.array(de_val_src), np.array(de_val_mt)]\n",
    "X_val_de = np.array(X_val).transpose()\n",
    "\n",
    "# Scores\n",
    "train_scores = np.array(de_train_scores).astype(float)\n",
    "y_train_de = train_scores\n",
    "\n",
    "val_scores = np.array(de_val_scores).astype(float)\n",
    "y_val_de = val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwFOE0_LPbL4"
   },
   "source": [
    "Define RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ze_fZHuDPlrD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IerDa2251swL"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exHbrWtq14jm"
   },
   "source": [
    "SVM have many parameters such as the kernel and the regularizating constant C. Here we will use C = 1 and compare kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "QiHCkGUgsJ8r",
    "outputId": "3a323764-7960-46f3-f41c-7785273f96a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.8815523248077068 Pearson 0.02316592901063899\n",
      "\n",
      "poly\n",
      "RMSE: 0.881270949278876 Pearson 0.025748763501295843\n",
      "\n",
      "rbf\n",
      "RMSE: 0.8811243881929659 Pearson 0.026738971503697792\n",
      "\n",
      "sigmoid\n",
      "RMSE: 23.256236575094878 Pearson 0.014059164328044008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "for k in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(X_train_de, y_train_de)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val_de)\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val_de)} Pearson {pearson[0]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg9YSBUG1zaL"
   },
   "source": [
    "#### Random Tree Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "VOld4zbmsOGL",
    "outputId": "9f403cb7-bb6c-4399-8e65-4a72283feacf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9171844096424996\n",
      "Pearson 0.020763124093085897 for n_estimators = 100\n",
      "RMSE: 0.9174139364155368\n",
      "Pearson 0.007309205571025664 for n_estimators = 500\n",
      "RMSE: 0.9165907358012123\n",
      "Pearson 0.009060574412041517 for n_estimators = 1000\n",
      "RMSE: 0.9168888726567898\n",
      "Pearson 0.007834303184429967 for n_estimators = 1500\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for n in [100, 500, 1000, 1500]:\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=666)\n",
    "\n",
    "    rf.fit(X_train_de, y_train_de)\n",
    "\n",
    "    predictions = rf.predict(X_val_de)\n",
    "\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print('RMSE:', rmse(predictions, y_val_de))\n",
    "    print(f\"Pearson {pearson[0]} for n_estimators = {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaqYBaKVfUpp"
   },
   "source": [
    "Here is a regressor using KerasRegressor. Makes use of Neural networks and hyper parameter searching. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8V30C4Sc69eY",
    "outputId": "93a29aaf-d2e0-4a60-dffc-52efcfe8aea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "RMSE: 0.864520518698789\n",
      "Pearson -0.004148737117413151, activation = sigmoid, size_1 = 80, size_2 = 40, optim = adam, batch size = 32\n",
      "RMSE: 0.8660208616282089\n",
      "Pearson 0.01558511976380477, activation = sigmoid, size_1 = 80, size_2 = 40, optim = adam, batch size = 64\n",
      "RMSE: 0.8640574886205751\n",
      "Pearson -0.007350409651671679, activation = sigmoid, size_1 = 80, size_2 = 40, optim = sgd, batch size = 32\n",
      "RMSE: 0.8641177376947687\n",
      "Pearson 0.03502894066692, activation = sigmoid, size_1 = 80, size_2 = 40, optim = sgd, batch size = 64\n",
      "RMSE: 0.8637814391950044\n",
      "Pearson 0.035974452790683696, activation = sigmoid, size_1 = 80, size_2 = 100, optim = adam, batch size = 32\n",
      "RMSE: 0.8640173516911003\n",
      "Pearson 0.014568127677929739, activation = sigmoid, size_1 = 80, size_2 = 100, optim = adam, batch size = 64\n",
      "RMSE: 0.8640883666689202\n",
      "Pearson 0.02295023370091999, activation = sigmoid, size_1 = 80, size_2 = 100, optim = sgd, batch size = 32\n",
      "RMSE: 0.8638008727251297\n",
      "Pearson 0.020900031018004233, activation = sigmoid, size_1 = 80, size_2 = 100, optim = sgd, batch size = 64\n",
      "RMSE: 0.8639560639146738\n",
      "Pearson 0.0343136161164313, activation = sigmoid, size_1 = 100, size_2 = 40, optim = adam, batch size = 32\n",
      "RMSE: 0.8639707633473074\n",
      "Pearson 0.022619654927871335, activation = sigmoid, size_1 = 100, size_2 = 40, optim = adam, batch size = 64\n",
      "RMSE: 0.8643287513963686\n",
      "Pearson 0.03582493405439792, activation = sigmoid, size_1 = 100, size_2 = 40, optim = sgd, batch size = 32\n",
      "RMSE: 0.8652495668012985\n",
      "Pearson -0.025705472295809714, activation = sigmoid, size_1 = 100, size_2 = 40, optim = sgd, batch size = 64\n",
      "RMSE: 0.8637812118725989\n",
      "Pearson 0.03048999158658789, activation = sigmoid, size_1 = 100, size_2 = 100, optim = adam, batch size = 32\n",
      "RMSE: 0.863781630907799\n",
      "Pearson 0.027920392906475765, activation = sigmoid, size_1 = 100, size_2 = 100, optim = adam, batch size = 64\n",
      "RMSE: 0.8637993215688445\n",
      "Pearson 0.017591345614779454, activation = sigmoid, size_1 = 100, size_2 = 100, optim = sgd, batch size = 32\n",
      "RMSE: 0.8638750992148851\n",
      "Pearson 0.02951970652735884, activation = sigmoid, size_1 = 100, size_2 = 100, optim = sgd, batch size = 64\n",
      "RMSE: 0.864703734185252\n",
      "Pearson 0.021662677313522834, activation = tanh, size_1 = 80, size_2 = 40, optim = adam, batch size = 32\n",
      "RMSE: 0.8642352075032683\n",
      "Pearson 0.025302089532449754, activation = tanh, size_1 = 80, size_2 = 40, optim = adam, batch size = 64\n",
      "RMSE: 0.8643421924101318\n",
      "Pearson 0.022060240113281608, activation = tanh, size_1 = 80, size_2 = 40, optim = sgd, batch size = 32\n",
      "RMSE: 0.8639268762640769\n",
      "Pearson -0.003323221040852464, activation = tanh, size_1 = 80, size_2 = 40, optim = sgd, batch size = 64\n",
      "RMSE: 0.8651167449894718\n",
      "Pearson -0.02867283311368187, activation = tanh, size_1 = 80, size_2 = 100, optim = adam, batch size = 32\n",
      "RMSE: 0.8639819006765488\n",
      "Pearson 0.03198039972634605, activation = tanh, size_1 = 80, size_2 = 100, optim = adam, batch size = 64\n",
      "RMSE: 0.863802363381035\n",
      "Pearson -0.015630037269075314, activation = tanh, size_1 = 80, size_2 = 100, optim = sgd, batch size = 32\n",
      "RMSE: 0.8640217969755749\n",
      "Pearson -0.03204223431226774, activation = tanh, size_1 = 80, size_2 = 100, optim = sgd, batch size = 64\n",
      "RMSE: 0.863844518180928\n",
      "Pearson 0.03347457923864474, activation = tanh, size_1 = 100, size_2 = 40, optim = adam, batch size = 32\n",
      "RMSE: 0.8637788983576932\n",
      "Pearson 0.02240380558187929, activation = tanh, size_1 = 100, size_2 = 40, optim = adam, batch size = 64\n",
      "RMSE: 0.8647636981570451\n",
      "Pearson -0.010839253163959908, activation = tanh, size_1 = 100, size_2 = 40, optim = sgd, batch size = 32\n",
      "RMSE: 0.8643304615886817\n",
      "Pearson 0.02373166965283564, activation = tanh, size_1 = 100, size_2 = 40, optim = sgd, batch size = 64\n",
      "RMSE: 0.8642455658462029\n",
      "Pearson -0.03215146021479718, activation = tanh, size_1 = 100, size_2 = 100, optim = adam, batch size = 32\n",
      "RMSE: 0.8638405192548667\n",
      "Pearson 0.02405493887014414, activation = tanh, size_1 = 100, size_2 = 100, optim = adam, batch size = 64\n",
      "RMSE: 0.8650365399356631\n",
      "Pearson -0.02852147987672634, activation = tanh, size_1 = 100, size_2 = 100, optim = sgd, batch size = 32\n",
      "RMSE: 0.8639911547229681\n",
      "Pearson 0.027585013475878942, activation = tanh, size_1 = 100, size_2 = 100, optim = sgd, batch size = 64\n"
     ]
    }
   ],
   "source": [
    "### Keras Regressor using Neural Networks ####\n",
    "\n",
    "# This is a neural network regressor that we built using keras\n",
    "\n",
    "# Import necessary packages\n",
    "from sklearn import linear_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Create random seed to replicate results\n",
    "seed = 1\n",
    "\n",
    "# Create lists that we can use for hyper-parameter search\n",
    "activation = ['sigmoid', 'tanh']  # activation functions used\n",
    "layer_size_1 = [80, 100]  # first layer size\n",
    "layer_size_2 = [40, 100]  # second layer size\n",
    "optimizer = ['adam', 'sgd']  # two optimizers used\n",
    "batch_size = [32, 64]  # different batch sizes\n",
    "\n",
    "# Conduct hyper-parameter search using 'for loops'\n",
    "\n",
    "for item in activation:\n",
    "    for size_1 in layer_size_1:\n",
    "        for size_2 in layer_size_2:\n",
    "            for optim in optimizer:\n",
    "                for batch in batch_size:\n",
    "\n",
    "                    # Build the neural network architecture. We add three layers into the neural network\n",
    "                    # and assign activation functions.\n",
    "                    def baseline_model():\n",
    "                        model = Sequential()\n",
    "                        model.add(Dense(40, input_dim=2, activation='tanh'))\n",
    "                        model.add(Dense(10, activation='tanh'))\n",
    "                        model.add(Dense(1, activation='linear'))\n",
    "                        model.compile(loss='mse', optimizer='adam')\n",
    "                        return model\n",
    "\n",
    "                    # Create the estimator using the KerasRegressor function from Keras and our\n",
    "                    # model architecutre\n",
    "                    estimator = KerasRegressor(\n",
    "                        build_fn=baseline_model, nb_epoch=100, batch_size=32, verbose=False, validation_split=0.2)\n",
    "\n",
    "                    # train the model with the training data\n",
    "                    estimator.fit(X_train_de, y_train_de)\n",
    "\n",
    "                    # Predict the model on the validation/test data\n",
    "                    predictions = estimator.predict(X_val_de)\n",
    "\n",
    "                    # compute pearson score for each hyper-parameter selection\n",
    "                    pearson = pearsonr(y_val_de, predictions)\n",
    "                    print('RMSE:', rmse(predictions, y_val_de))\n",
    "                    print(\n",
    "                        f\"Pearson {pearson[0]}, activation = {item}, size_1 = {size_1}, size_2 = {size_2}, optim = {optim}, batch size = {batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjKaqYNNfdnj"
   },
   "source": [
    "Here I test out a Kernel Ridge Regressor. Similar to SVR with slight differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "2trZRJByAcVC",
    "outputId": "1c7f62de-cfe2-4b6e-d6b6-91c087bd6e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8638932073906487\n",
      "Pearson score for KernelRidge Regression: 0.03584678337960797 for kernel = linear\n",
      "RMSE: 0.8638899984186302\n",
      "Pearson score for KernelRidge Regression: 0.03318011280305015 for kernel = poly\n",
      "RMSE: 0.8638932466690457\n",
      "Pearson score for KernelRidge Regression: 0.035468538582623894 for kernel = rbf\n",
      "RMSE: 0.8638994229750283\n",
      "Pearson score for KernelRidge Regression: 0.0387287287343936 for kernel = sigmoid\n"
     ]
    }
   ],
   "source": [
    "### Kernel Ridge Regressor ####\n",
    "\n",
    "# import necessary packages to apply KernelRidge Regressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Create list for kernel such that we can perform hyper-parameter search.\n",
    "for k in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    # Create Regressor\n",
    "    kr = KernelRidge(alpha=0.2, kernel=k)\n",
    "\n",
    "    # Train the model on the trianing data\n",
    "    kr.fit(X_train_de, y_train_de)\n",
    "\n",
    "    # Predict outcome for the validation/test data\n",
    "    predictions = kr.predict(X_val_de)\n",
    "\n",
    "    # Compute and pring pearson value for each hyper-parameter selection\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print('RMSE:', rmse(predictions, y_val_de))\n",
    "    print(\n",
    "        f\"Pearson score for KernelRidge Regression: {pearson[0]} for kernel = {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qpv0rLpiflJr"
   },
   "source": [
    "The next regressor I test is the Passive Aggressive Regressor from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Xs49RXhaBtdS",
    "outputId": "aeae3b8b-3c4a-4023-dc5f-ee3a7c8427f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8638994229750283\n",
      "Pearson score for PA Regression: 0.0387287287343936, max iteration is 100\n",
      "RMSE: 0.8638994229750283\n",
      "Pearson score for PA Regression: 0.0387287287343936, max iteration is 200\n",
      "RMSE: 0.8638994229750283\n",
      "Pearson score for PA Regression: 0.0387287287343936, max iteration is 300\n"
     ]
    }
   ],
   "source": [
    "### Passive Aggressive Regressor ###\n",
    "\n",
    "# import necessary packages for the Passive Aggressive regressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "\n",
    "# create values for max iterations used for hyper-parameter search\n",
    "max_iter = [100, 200, 300]\n",
    "\n",
    "# conduct hyper-parameter search\n",
    "for value in max_iter:\n",
    "\n",
    "    # create regressor from library\n",
    "    clf = PassiveAggressiveRegressor(max_iter=value, random_state=0)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    clf.fit(X_train_de, y_train_de)\n",
    "\n",
    "    # predict outcomes for validation/test data\n",
    "    clf.predict(X_val_de)\n",
    "\n",
    "    # Print Pearson score for each hyper-parameter selection\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print('RMSE:', rmse(predictions, y_val_de))\n",
    "    print(\n",
    "        f\"Pearson score for PA Regression: {pearson[0]}, max iteration is {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kmCMJmIfrfI"
   },
   "source": [
    "I also try the TheilSen regressor from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Zw1UyTZEFtsX",
    "outputId": "aa14544c-5e5b-4f64-ff36-777e5a8762ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8638994229750283\n",
      "Pearson score for TSR Regression: 0.0387287287343936, max iteration = 100\n",
      "RMSE: 0.8638994229750283\n",
      "Pearson score for TSR Regression: 0.0387287287343936, max iteration = 200\n",
      "RMSE: 0.8638994229750283\n",
      "Pearson score for TSR Regression: 0.0387287287343936, max iteration = 300\n"
     ]
    }
   ],
   "source": [
    "### TheilSen Regressor ###\n",
    "\n",
    "# Import necessary packages for the TheilSen Regressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "# create values for max iterations used for hyper-parameter search\n",
    "max_iter = [100, 200, 300]\n",
    "\n",
    "# conduct hyper-parameter search\n",
    "for value in max_iter:\n",
    "\n",
    "    # create regressor from library\n",
    "    tsr = TheilSenRegressor(max_iter=value, random_state=0)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    tsr.fit(X_train_de, y_train_de)\n",
    "\n",
    "    # predict outcomes for validation/test data\n",
    "    tsr.predict(X_val_de)\n",
    "\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print('RMSE:', rmse(predictions, y_val_de))\n",
    "    print(\n",
    "        f\"Pearson score for TSR Regression: {pearson[0]}, max iteration = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1l1cERjf0ir"
   },
   "source": [
    "Next I test the Gradient Boosting regressor from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "V1_e__2thJgI",
    "outputId": "f766b5d9-f9eb-4ae5-c399-ec55159210f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8639040160750084\n",
      "Pearson Gradient Boosting Regressor: nan with lr = 0.001, max_depth = 1, n_estimator = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8639071947381184\n",
      "Pearson Gradient Boosting Regressor: nan with lr = 0.001, max_depth = 1, n_estimator = 500\n",
      "RMSE: 0.8637188063364276\n",
      "Pearson Gradient Boosting Regressor: 0.04102789914741826 with lr = 0.001, max_depth = 2, n_estimator = 200\n",
      "RMSE: 0.8636124658386475\n",
      "Pearson Gradient Boosting Regressor: 0.03539659808635271 with lr = 0.001, max_depth = 2, n_estimator = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8639027377528806\n",
      "Pearson Gradient Boosting Regressor: nan with lr = 0.0005, max_depth = 1, n_estimator = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.863904610309668\n",
      "Pearson Gradient Boosting Regressor: nan with lr = 0.0005, max_depth = 1, n_estimator = 500\n",
      "RMSE: 0.8637630797945611\n",
      "Pearson Gradient Boosting Regressor: 0.05052686627412537 with lr = 0.0005, max_depth = 2, n_estimator = 200\n",
      "RMSE: 0.863702224295285\n",
      "Pearson Gradient Boosting Regressor: 0.03788722301305036 with lr = 0.0005, max_depth = 2, n_estimator = 500\n"
     ]
    }
   ],
   "source": [
    "# import necessary package from sklearn\n",
    "\n",
    "from sklearn import ensemble\n",
    "\n",
    "#### Gradient Boosting Regressor ####\n",
    "\n",
    "# create lists for hyper-parameter search. Change learning rate,\n",
    "# max depth and n_estimator value\n",
    "\n",
    "for lr in [0.001, 0.0005]:\n",
    "    for max_depth in [1, 2]:\n",
    "        for n_estimator in [200, 500]:\n",
    "\n",
    "            # Put hyper-parameters in parameter dictionary\n",
    "            params = {'n_estimators': n_estimator, 'max_depth': max_depth, 'min_samples_split': 2,\n",
    "                      'learning_rate': lr, 'loss': 'ls'}\n",
    "\n",
    "            # Create Gradient Boosting Regressor\n",
    "            clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "            # Train the model on the training data\n",
    "            clf.fit(X_train_de, y_train_de)\n",
    "\n",
    "            # Predict outcomes on validation data\n",
    "            predictions = clf.predict(X_val_de)\n",
    "\n",
    "            # Compute and print pearson score for each hyper-parameter evaluation\n",
    "            pearson = pearsonr(y_val_de, predictions)\n",
    "            print('RMSE:', rmse(predictions, y_val_de))\n",
    "            print(\n",
    "                f\"Pearson Gradient Boosting Regressor: {pearson[0]} with lr = {lr}, max_depth = {max_depth}, n_estimator = {n_estimator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckuKf4_Wf5nU"
   },
   "source": [
    "Here I use an MLP regressor from sklearn and a pipeline which is used to create a hyper parameter search. So far the best results have come from this of about 0.772."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "tVC4sO6MhWiC",
    "outputId": "b7ce20f3-e92d-4454-e003-a2d9071630c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8627738173589881\n",
      "mlp Regression Pearson: 0.04871467066315298 for hidden_layer_size = 100, activation = relu, learning_rate = 0.001, solver = adam\n",
      "RMSE: 0.8640966324587362\n",
      "mlp Regression Pearson: 0.01666724969930131 for hidden_layer_size = 100, activation = relu, learning_rate = 0.001, solver = sgd\n",
      "RMSE: 0.8636248006571877\n",
      "mlp Regression Pearson: 0.03147185820197343 for hidden_layer_size = 100, activation = logistic, learning_rate = 0.001, solver = adam\n",
      "RMSE: 0.8638408517197841\n",
      "mlp Regression Pearson: 0.023327163080269296 for hidden_layer_size = 100, activation = logistic, learning_rate = 0.001, solver = sgd\n",
      "RMSE: 0.8660894003533958\n",
      "mlp Regression Pearson: 0.00520628517702393 for hidden_layer_size = 100, activation = tanh, learning_rate = 0.001, solver = adam\n",
      "RMSE: 0.8638373858946172\n",
      "mlp Regression Pearson: 0.01875658845426788 for hidden_layer_size = 100, activation = tanh, learning_rate = 0.001, solver = sgd\n",
      "RMSE: 0.8650959316659498\n",
      "mlp Regression Pearson: 0.018673115547737158 for hidden_layer_size = (100, 100), activation = relu, learning_rate = 0.001, solver = adam\n",
      "RMSE: 0.8642289942250485\n",
      "mlp Regression Pearson: 0.012904834313285204 for hidden_layer_size = (100, 100), activation = relu, learning_rate = 0.001, solver = sgd\n",
      "RMSE: 0.8636781550697766\n",
      "mlp Regression Pearson: 0.02633209157850886 for hidden_layer_size = (100, 100), activation = logistic, learning_rate = 0.001, solver = adam\n",
      "RMSE: 0.8644449283818799\n",
      "mlp Regression Pearson: 0.03274380596571168 for hidden_layer_size = (100, 100), activation = logistic, learning_rate = 0.001, solver = sgd\n",
      "RMSE: 0.864093667158875\n",
      "mlp Regression Pearson: 0.022504387796655527 for hidden_layer_size = (100, 100), activation = tanh, learning_rate = 0.001, solver = adam\n",
      "RMSE: 0.8642290880472124\n",
      "mlp Regression Pearson: 0.008519186136853428 for hidden_layer_size = (100, 100), activation = tanh, learning_rate = 0.001, solver = sgd\n",
      "highest pearson score is 0.04871467066315298\n"
     ]
    }
   ],
   "source": [
    "# MLP regressor.... using neural networks to predict labels #\n",
    "\n",
    "# import necessary packages from sklearn library for MLP regressor.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create list of scores so we can keep track of best Pearson score\n",
    "list_of_scores = []\n",
    "\n",
    "# initalise hyper-parameter search\n",
    "for hidden_layer_size in [(100), (100, 100)]:\n",
    "    for activation in ['relu', 'logistic', 'tanh']:\n",
    "        for learning_rate in [0.001]:\n",
    "            for solver in ['adam', 'sgd']:\n",
    "\n",
    "                # use pipeline to create MLP regressor\n",
    "                mlp = make_pipeline(StandardScaler(),\n",
    "                                    MLPRegressor(hidden_layer_sizes=hidden_layer_size,\n",
    "                                                 tol=1e-2, max_iter=500, random_state=0, early_stopping=False, learning_rate_init=learning_rate, activation=activation, solver=solver))\n",
    "\n",
    "                # Train model on the training data\n",
    "                mlp.fit(X_train_de, y_train_de)\n",
    "\n",
    "                # Evaluate model on training/validation data\n",
    "                predictions = mlp.predict(X_val_de)\n",
    "\n",
    "                # Compute and print Pearson score for each hyper-parameter evaluation\n",
    "                pearson = pearsonr(y_val_de, predictions)\n",
    "\n",
    "                list_of_scores.append(pearson[0])\n",
    "\n",
    "                print('RMSE:', rmse(predictions, y_val_de))\n",
    "                print(\n",
    "                    f\"mlp Regression Pearson: {pearson[0]} for hidden_layer_size = {hidden_layer_size}, activation = {activation}, learning_rate = {learning_rate}, solver = {solver}\")\n",
    "\n",
    "# Print highest pearson score\n",
    "highest_pearson = max(list_of_scores)\n",
    "print(f'highest pearson score is {highest_pearson}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BAhlHdBgNh8"
   },
   "source": [
    "Here is a neural network regressor which I create from scratch using pytorch. I have included a small hyper parameter search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9puD_0zkC2c"
   },
   "source": [
    "### Writing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQvvIhPDkUnR"
   },
   "source": [
    "Here is our function to write the scores into a txt file. We can follow the <Method> <ID> <SCORE> template but having only the scores will work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LN3NtkF4kPxw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def writeScores(method_name, scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx, x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            # print(out)\n",
    "            output_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzTj3-iW28wY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVss_RLBkFei"
   },
   "outputs": [],
   "source": [
    "# EN-DE\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "############FLAIR EMBEDDINGS##########\n",
    "# de_test_src = get_embeddings(\"./test.ende.src\",document_embeddings_en,'en')\n",
    "# de_test_mt = get_embeddings(\"./test.ende.mt\",document_embeddings_de,'de')\n",
    "\n",
    "############BERT EMBEDDINGS##########\n",
    "de_test_src = get_bert_embeddings(\"./test.ende.src\", 'en', pooling_fcn=None)\n",
    "de_test_mt = get_bert_embeddings(\"./test.ende.mt\", 'de', pooling_fcn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Xkoc4FHiTJR"
   },
   "outputs": [],
   "source": [
    "# Compiling test data into tensor form to be fed into Neural net\n",
    "\n",
    "num_samples = len(de_test_src)\n",
    "num_dims = len(de_test_src[0]) * 2\n",
    "X_test = torch.zeros((num_samples, num_dims), dtype=torch.float)\n",
    "for i in range(len(de_test_src)):\n",
    "    en_vec = de_test_src[i]\n",
    "    de_vec = de_test_mt[i]\n",
    "    vec = np.concatenate((en_vec, de_vec))\n",
    "    vec = torch.tensor(vec, dtype=torch.float).squeeze()\n",
    "    X_test[i, :] = vec\n",
    "\n",
    "X_test_de = X_test\n",
    "\n",
    "# Obtain predictions\n",
    "predictions = test(model, X_test.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWnNUR0Gku_9"
   },
   "outputs": [],
   "source": [
    "# Writing submisison files\n",
    "\n",
    "from google.colab import files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "writeScores(\"korbi_bert\", predictions.squeeze())\n",
    "\n",
    "with ZipFile(\"en-de_kbert.zip\", \"w\") as newzip:\n",
    "    newzip.write(\"predictions.txt\")\n",
    "\n",
    "files.download('en-de_kbert.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXoDKREdhVBn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "AlF9jpCDguuV"
   ],
   "machine_shape": "hm",
   "name": "NLPCoursework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
